# Distillation_project
This project uses knowledge distillation to train a smaller student model with guidance from a larger teacher model. It combines hard labels, soft logits, and feature alignment losses to improve the studentâ€™s accuracy and efficiency for image classification tasks.
